{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47b6ebe0-b391-4c66-a3e2-6632f9e9dc1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install langchain==0.0.179 pydantic tiktoken chromadb pypdf Xformers InstructorEmbedding sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f118a6-1658-45d0-9ca8-122508f5be48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.179\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://www.github.com/hwchase17/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2c31edd-73ed-43e4-8fd5-bd34d3db4406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace new_papers/new_papers/toolformer.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://www.dropbox.com/s/zoj9rnm7oyeaivb/new_papers.zip\n",
    "!unzip -q new_papers.zip -d new_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7667381-31fe-4e54-8bd7-dd87c73db849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "from random import choice\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f681a1e-eda0-4018-878f-da695b57b4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('api')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logHandler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(logHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b97aa9-bf3f-459c-80f6-0827faa3c4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_model(model_name: str, cache_dir: str = None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        low_cpu_mem_usage=True,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "931f2782-f4db-4699-b9e2-c067a9695ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so'), PosixPath('/home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efe66e26e3c4d4ea4d2ad4299e11b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='TheBloke/wizardLM-7B-HF'\n",
    "cache_dir='/home/ec2-user/SageMaker/.cache'\n",
    "tokenizer, model = setup_model(model_name, cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e0453c-aca8-4753-9c7a-05ea4b9c001c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 25 13:32:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   25C    P0    61W / 300W |   7931MiB / 23028MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1136      C   ...vs/pytorch_p39/bin/python     7929MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb6c252-a7a6-4d6a-a47f-13c489f8da46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task='text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    max_length=1024,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f85157c-8991-4104-bb06-3ec8dbbae28c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "London is the capital city of England.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(llm('What is the capital of England?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931663c2-7b74-4d74-b20d-a316a24b86ee",
   "metadata": {},
   "source": [
    "## Setup Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee3f3b52-0c71-4fa9-82ee-789dbc0a3c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2fffced-2383-44c6-b12f-9d775798c966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('./new_papers/new_papers/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3235476-db07-4329-abd4-130369a7787e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d22b9d0-d12d-4833-95d9-83bee9b7cfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482a9b04-a00a-4006-a029-f6177b521c8d",
   "metadata": {},
   "source": [
    "## Embbeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01196df5-4291-48ff-842b-0091a2b56372",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b20801e5324d439d7fce7b689a15e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7f436/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f095e2f0a2466baf3f61e6524109e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f99e0b764aa416e858ca3d3d9cf0b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1261cb1bb6e04367851df991d2732526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db19e9d6f19489b9e3c176b9d33eec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0daf57f436/README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2055bea2d04650a68f862bf6a688f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)af57f436/config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27f70e3f3514cbcb953f50516be694a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea504ec82504131b3516cfabb5f3f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182b10f3e4fd4084ad08759ea81171bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94038399c6a646628a93715b56bc199c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e728bfb68342a7a13969242b9dc8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0090e3066df4ad6b288eaaeb939d284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7f436/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a84f2291d04ee2bef297ab3890cfe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb3cc2428224b02b3d92b41331acebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)f57f436/modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "instructor_embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-xl\", \n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b40b6d-7e8f-4aef-85e0-136a206fad8c",
   "metadata": {},
   "source": [
    "## Create DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8db05d31-94e5-41cf-95d8-19a6c2e965c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding = instructor_embeddings\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=texts, \n",
    "    embedding=embedding,\n",
    "    persist_directory='db',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f0eab-4d17-463a-a1c0-256ed2906d96",
   "metadata": {},
   "source": [
    "## Set chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92778c5d-2e15-4b88-b1af-bb72bc895546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3baaec6c-c50f-44fd-a102-b78cec734e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b85930eb-fec5-4392-ba2f-67ea298d8ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    lines = text.split('\\n')\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae6c0f-6938-49d5-836f-77894e610a23",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "011090db-d19a-45e2-9f73-cf4f5cdfbc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FlashAttention is a new attention algorithm proposed by the authors that reduces the number of memory\n",
      "reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. It achieves this by splitting the\n",
      "input into blocks and making several passes over them, and by storing the softmax normalization factor from\n",
      "the forward pass to quickly recompute attention on-chip in the backward pass. FlashAttention also extends to\n",
      "block-sparse attention, yielding an approximate attention algorithm that is faster than any existing\n",
      "approximate attention method.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/Flash-attention.pdf\n",
      "new_papers/new_papers/Flash-attention.pdf\n",
      "new_papers/new_papers/Flash-attention.pdf\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What is Flash attention?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8da873c-9ca1-4f08-9402-e85f30bb6c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " IO-aware refers to a method or approach that takes into account the input/output (I/O) performance when\n",
      "designing or optimizing a system. In the context of deep learning, it means considering the impact of the\n",
      "computational load on the input/output devices (such as CPUs, GPUs, or other accelerators) during the training\n",
      "process.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/Flash-attention.pdf\n",
      "new_papers/new_papers/Flash-attention.pdf\n",
      "new_papers/new_papers/Flash-attention.pdf\n",
      "CPU times: user 9.63 s, sys: 0 ns, total: 9.63 s\n",
      "Wall time: 9.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What does IO-aware mean?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "222eb192-a0df-4ce3-b84c-3166a7d76999",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Toolformer is a language model that has been trained to use external tools such as search engines,\n",
      "calculators, and translation systems via simple API calls. It was introduced in a recent research paper titled\n",
      "\"Learning to Use Tools with Language Models\" by Timo Schick, Jane Dwivedi-Yu, Roberto Dessìy, Roberta\n",
      "Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Meta AI ResearchyUniversitat Pompeu\n",
      "Fabra.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/toolformer.pdf\n",
      "new_papers/new_papers/toolformer.pdf\n",
      "new_papers/new_papers/toolformer.pdf\n",
      "CPU times: user 15.1 s, sys: 0 ns, total: 15.1 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What is toolformer?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "762440fa-2c99-4dda-9bb1-9bbe9dbf29a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toolformer can be used with any external tool that has a simple API and provides relevant information for the\n",
      "given task. For example, it could be used with search engines like Google or Bing, calculators like Wolfram\n",
      "Alpha or Mathway, or translation systems like Google Translate or Microsoft Translator.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/toolformer.pdf\n",
      "new_papers/new_papers/toolformer.pdf\n",
      "new_papers/new_papers/toolformer.pdf\n",
      "CPU times: user 8.76 s, sys: 3.68 ms, total: 8.76 s\n",
      "Wall time: 8.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What tools can be used with toolformer?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7e081ad-6fa6-40b9-8094-f5fa6ee360a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are different types of retrieval augmentations for LLMs, including dense and sparse retrievers. Dense\n",
      "retrievers work with dense queries and dense document representations, while sparse retrievers use sparse bag-\n",
      "of-words representations of the documents and queries. Both approaches have their advantages and\n",
      "disadvantages, and the choice depends on the specific task and dataset. Additionally, grounding the\n",
      "predictions through tools such as calculators can increase the truthfulness of the generated responses.\n",
      "Estimating and reducing uncertainty is another direction to explore, as it can help LMMs learn what they know\n",
      "and what they don't. Finally, allowing LMMs to leverage external tools can also improve their performance,\n",
      "especially if the missing information is crucial for the task. Overall, the best retrieval augmentations\n",
      "depend on the specific requirements of the task and should be evaluated based on their effectiveness and\n",
      "efficiency.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/Augmenting LLMs Survey.pdf\n",
      "new_papers/new_papers/Augmenting LLMs Survey.pdf\n",
      "new_papers/new_papers/Augmenting LLMs Survey.pdf\n",
      "CPU times: user 24.8 s, sys: 3.55 ms, total: 24.8 s\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "query = \"What are the best retrieval augmentations for LLMs?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8953a4ac-d30d-45a7-8a3f-519bac8116a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REALM (Guu et al., 2020) and RAG (Lewis et al., 2020) are both methods that use retrieval-augmented language\n",
      "models to improve the performance of question answering systems. However, there are some key differences\n",
      "between them. REALM\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/Augmenting LLMs Survey.pdf\n",
      "new_papers/new_papers/Augmenting LLMs Survey.pdf\n",
      "new_papers/new_papers/ReACT.pdf\n",
      "CPU times: user 8.51 s, sys: 0 ns, total: 8.51 s\n",
      "Wall time: 8.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What are the differences between REALM and RAG?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69132fa5-f10b-4bc0-a31f-4fbac7de465c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('similarity', <langchain.vectorstores.chroma.Chroma at 0x7f2e71b13eb0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.retriever.search_type , qa_chain.retriever.vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "981bcf1f-1f9d-45d0-a874-8261b594a192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
