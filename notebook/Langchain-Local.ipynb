{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b6ebe0-b391-4c66-a3e2-6632f9e9dc1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install -U langchain pydantic chromadb pypdf sentence_transformers Xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f118a6-1658-45d0-9ca8-122508f5be48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.0.179\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://www.github.com/hwchase17/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, numexpr, numpy, openapi-schema-pydantic, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2c31edd-73ed-43e4-8fd5-bd34d3db4406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace new_papers/new_papers/toolformer.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://www.dropbox.com/s/zoj9rnm7oyeaivb/new_papers.zip\n",
    "!unzip -q new_papers.zip -d new_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7667381-31fe-4e54-8bd7-dd87c73db849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "from random import choice\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f681a1e-eda0-4018-878f-da695b57b4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('api')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logHandler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(logHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b97aa9-bf3f-459c-80f6-0827faa3c4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_model(model_name: str, cache_dir: str = None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        low_cpu_mem_usage=True,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "931f2782-f4db-4699-b9e2-c067a9695ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so'), PosixPath('/home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf41dc8161894ce695c82ae21e930e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='TheBloke/wizardLM-7B-HF'\n",
    "cache_dir='/home/ec2-user/SageMaker/.cache'\n",
    "tokenizer, model = setup_model(model_name, cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e0453c-aca8-4753-9c7a-05ea4b9c001c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 25 14:40:34 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   26C    P0    56W / 300W |   7931MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     20576      C   ...vs/pytorch_p39/bin/python     7929MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bb6c252-a7a6-4d6a-a47f-13c489f8da46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task='text-generation',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    max_length=2048,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f85157c-8991-4104-bb06-3ec8dbbae28c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "London is the capital city of England.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(llm('What is the capital of England?'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "931663c2-7b74-4d74-b20d-a316a24b86ee",
   "metadata": {},
   "source": [
    "## Setup Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee3f3b52-0c71-4fa9-82ee-789dbc0a3c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d2fffced-2383-44c6-b12f-9d775798c966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('./new_papers/new_papers/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3235476-db07-4329-abd4-130369a7787e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d22b9d0-d12d-4833-95d9-83bee9b7cfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "482a9b04-a00a-4006-a029-f6177b521c8d",
   "metadata": {},
   "source": [
    "## Embbeding (SentenceBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01196df5-4291-48ff-842b-0091a2b56372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7df0a63a-1636-4723-ac2e-5ee25b74e935",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04895173758268356,\n",
       " -0.039861973375082016,\n",
       " -0.021562790498137474,\n",
       " 0.009908495470881462,\n",
       " -0.03810397535562515]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "06b61e98-2a8f-4a9c-aaae-0b5ca39ed667",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04895175248384476,\n",
       " -0.03986193612217903,\n",
       " -0.02156277559697628,\n",
       " 0.009908493608236313,\n",
       " -0.03810398280620575]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result = embeddings.embed_documents([text, \"This is not a test document.\"])\n",
    "doc_result[0][:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6b40b6d-7e8f-4aef-85e0-136a206fad8c",
   "metadata": {},
   "source": [
    "## Create Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8db05d31-94e5-41cf-95d8-19a6c2e965c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=texts, \n",
    "    embedding=embeddings,\n",
    "    persist_directory='db',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "781f0eab-4d17-463a-a1c0-256ed2906d96",
   "metadata": {},
   "source": [
    "## Set chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92778c5d-2e15-4b88-b1af-bb72bc895546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3baaec6c-c50f-44fd-a102-b78cec734e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69132fa5-f10b-4bc0-a31f-4fbac7de465c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('similarity', <langchain.vectorstores.chroma.Chroma at 0x7f2ed8fa3460>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.retriever.search_type , qa_chain.retriever.vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "981bcf1f-1f9d-45d0-a874-8261b594a192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "logger.info(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b85930eb-fec5-4392-ba2f-67ea298d8ad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrap_text_preserve_newlines(text, width=110):\n",
    "    lines = text.split('\\n')\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "    return wrapped_text\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    logger.info(wrap_text_preserve_newlines(llm_response['result']))\n",
    "    logger.info('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        logger.info(source.metadata['source'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76ae6c0f-6938-49d5-836f-77894e610a23",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "011090db-d19a-45e2-9f73-cf4f5cdfbc3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Flash attention is a type of self-attention mechanism used in neural networks for natural language processing\n",
      "tasks such as machine translation and text classification. It was introduced in the paper \"Fast and Accurate\n",
      "Machine Translation by Predicting Neural Networks\" by Vaswani et al. in 2017. Flash attention allows the model\n",
      "to quickly focus on important parts of the input sequence while ignoring less relevant parts, which can\n",
      "improve performance on long sequences.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/Flash-attention.pdf\n",
      "new_papers/new_papers/Flash-attention.pdf\n",
      "new_papers/new_papers/Flash-attention.pdf\n",
      "CPU times: user 13.7 s, sys: 2.56 ms, total: 13.7 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What is Flash attention?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8da873c-9ca1-4f08-9402-e85f30bb6c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " IO-awareness refers to designing computer systems that are aware of their input/output (I/O) capabilities and\n",
      "limitations. It involves optimizing software and hardware components to minimize I/O latency and maximize\n",
      "throughput. In the context of deep learning, IO-awareness means designing algorithms and architectures that\n",
      "take into account the I/O bottleneck caused by large amounts of data being transferred between memory and\n",
      "storage devices.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/Flash-attention.pdf\n",
      "new_papers/new_papers/Flash-attention.pdf\n",
      "new_papers/new_papers/Flash-attention.pdf\n",
      "CPU times: user 12.6 s, sys: 0 ns, total: 12.6 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What does IO-aware mean?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "222eb192-a0df-4ce3-b84c-3166a7d76999",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Toolformer is a natural language processing (NLP) model developed by OpenAI that uses external tools such as\n",
      "search engines, calculators, and calendar s to complete tasks such as answering questions and performing\n",
      "calculations. It is designed to improve upon the limitations of today's language models by giving them the\n",
      "ability to use external resources to better understand and respond to user input.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/Augmenting LLMs Survey.pdf\n",
      "new_papers/new_papers/toolformer.pdf\n",
      "new_papers/new_papers/toolformer.pdf\n",
      "CPU times: user 10.2 s, sys: 0 ns, total: 10.2 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What is toolformer?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "762440fa-2c99-4dda-9bb1-9bbe9dbf29a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toolformer can be used with any external tool that has a simple API and provides relevant information for the\n",
      "given task. For example, it could be used with search engines like Google or Bing, calculators like Wolfram\n",
      "Alpha or Mathway, or translation systems like Google Translate or Microsoft Translator.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/toolformer.pdf\n",
      "new_papers/new_papers/toolformer.pdf\n",
      "new_papers/new_papers/toolformer.pdf\n",
      "CPU times: user 8.76 s, sys: 3.68 ms, total: 8.76 s\n",
      "Wall time: 8.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What tools can be used with toolformer?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7e081ad-6fa6-40b9-8094-f5fa6ee360a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are different types of retrieval augmentations for LLMs, including dense and sparse retrievers. Dense\n",
      "retrievers work with dense queries and dense document representations, while sparse retrievers use sparse bag-\n",
      "of-words representations of the documents and queries. Both approaches have their advantages and\n",
      "disadvantages, and the choice depends on the specific task and dataset. Additionally, grounding the\n",
      "predictions through tools such as calculators can increase the truthfulness of the generated responses.\n",
      "Estimating and reducing uncertainty is another direction to explore, as it can help LMMs learn what they know\n",
      "and what they don't. Finally, allowing LMMs to leverage external tools can also improve their performance,\n",
      "especially if the missing information is crucial for the task. Overall, the best retrieval augmentations\n",
      "depend on the specific requirements of the task and should be evaluated based on their effectiveness and\n",
      "efficiency.\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/Augmenting LLMs Survey.pdf\n",
      "new_papers/new_papers/Augmenting LLMs Survey.pdf\n",
      "new_papers/new_papers/Augmenting LLMs Survey.pdf\n",
      "CPU times: user 24.8 s, sys: 3.55 ms, total: 24.8 s\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "query = \"What are the best retrieval augmentations for LLMs?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8953a4ac-d30d-45a7-8a3f-519bac8116a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REALM (Guu et al., 2020) and RAG (Lewis et al., 2020) are both methods that use retrieval-augmented language\n",
      "models to improve the performance of question answering systems. However, there are some key differences\n",
      "between them. REALM\n",
      "\n",
      "\n",
      "Sources:\n",
      "new_papers/new_papers/Augmenting LLMs Survey.pdf\n",
      "new_papers/new_papers/Augmenting LLMs Survey.pdf\n",
      "new_papers/new_papers/ReACT.pdf\n",
      "CPU times: user 8.51 s, sys: 0 ns, total: 8.51 s\n",
      "Wall time: 8.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"What are the differences between REALM and RAG?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04bcb7-ba7c-45f9-b4fa-391fc01ef5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
