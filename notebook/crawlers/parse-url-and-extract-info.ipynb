{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b45b0cb6-a78b-4881-9567-7dc59f9aa785",
   "metadata": {},
   "source": [
    "# Crawl web page and extract information\n",
    "\n",
    "- 웹페이지 주소를 입력해서 텍스트 데이터만 추출\n",
    "- 텍스트 데이터에서 LLM 을 이용하여 정보를 추출\n",
    "- 다른 작업에 사용할 수 있도록 jsonl 형태로 내보내기\n",
    "\n",
    "---\n",
    "\n",
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f171286-602e-4129-ac74-4c61f28a5f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U requests beautifulsoup4 trafilatura tiktoken langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55528aec-2132-45d9-a32e-102516344cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# web scraping\n",
    "import requests\n",
    "import trafilatura\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# sagemaker inference\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# splitter\n",
    "import tiktoken\n",
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "66960236-deeb-4d1b-83e9-65fd1d7a4925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    'https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/acl.html',\n",
    "    'https://www.imdb.com/title/tt3748528/plotsummary/',\n",
    "    'https://starwars.fandom.com/wiki/Andor_(television_series)',\n",
    "]\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74245ef2-8667-4398-b743-b3182f642cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fallback_parse(response_content):\n",
    "    soup = BeautifulSoup(response_content, 'html.parser')\n",
    "    text = soup.find_all(string=True)\n",
    "    cleaned_text = ''\n",
    "    blacklist = [\n",
    "        '[document]',\n",
    "        'noscript',\n",
    "        'header',\n",
    "        'html',\n",
    "        'meta',\n",
    "        'head', \n",
    "        'input',\n",
    "        'script',\n",
    "        'style',]\n",
    "\n",
    "    for item in text:\n",
    "        if item.parent.name not in blacklist:\n",
    "            cleaned_text += '{} '.format(item)\n",
    "            \n",
    "    cleaned_text = cleaned_text.replace('\\t', '')\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47110ec7-8447-4b9c-9fe8-bb918733d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(url):    \n",
    "    print(f'parse url: {url}...')\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "\n",
    "    contents = trafilatura.extract(\n",
    "        downloaded, output_format=\"json\",\n",
    "        include_comments=False, include_links=False, with_metadata=True,\n",
    "        date_extraction_params={'extensive_search': True, 'original_date': True},\n",
    "    )\n",
    "    \n",
    "    if contents:\n",
    "        json_output = json.loads(contents)\n",
    "        return json_output['text']\n",
    "    else:\n",
    "        try:\n",
    "            resp = requests.get(url)\n",
    "            if resp.status_code == 200:\n",
    "                return fallback_parse(resp.content)\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea1151-02d7-4562-be5b-86e4f3dd29f0",
   "metadata": {},
   "source": [
    "# 1. Crawl url and extract texts\n",
    "\n",
    "- trafilatura 를 이용하여 url 에서 텍스트만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c44a7fff-1673-4497-932d-f5694943f43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse url: https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/acl.html...\n",
      "len: 9100\n",
      "\n",
      "parse url: https://www.imdb.com/title/tt3748528/plotsummary/...\n",
      "len: 16040\n",
      "\n",
      "parse url: https://starwars.fandom.com/wiki/Andor_(television_series)...\n",
      "len: 4801\n",
      "\n",
      "num docs: 3\n",
      "CPU times: user 1.55 s, sys: 63.1 ms, total: 1.61 s\n",
      "Wall time: 4.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "docs = []\n",
    "for url in urls:\n",
    "    doc = parse(url)\n",
    "    print(f'len: {len(doc)}\\n')\n",
    "    if doc is None:\n",
    "        print(f'failed to parse url: {url}')\n",
    "        continue\n",
    "    docs.append(doc)\n",
    "    \n",
    "print(f'num docs: {len(docs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f9a2219-59ec-4e55-8d8f-622644cdbbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0:\n",
      "Anti-corruption layer pattern Intent The anti-corruption layer ...\n",
      "\n",
      "doc 1:\n",
      "- In a time of conflict, a group of unlikely heroes band together ...\n",
      "\n",
      "doc 2:\n",
      "- \"We've all done terrible things on behalf of the Rebellion. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    text = textwrap.shorten(doc, width=70, placeholder=' ...\\n')\n",
    "    print(f'doc {i}:\\n{text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521a015-9695-4d49-a912-f02413c372b9",
   "metadata": {},
   "source": [
    "# 2. Extract information\n",
    "\n",
    "- [Sagemaker Jumpstart](https://aws.amazon.com/blogs/machine-learning/llama-2-foundation-models-from-meta-are-now-available-in-amazon-sagemaker-jumpstart/) 를 이용하여 Llama2 배포\n",
    "- Llama2 를 이용하여 정보를 추출한다\n",
    "\n",
    "---\n",
    "\n",
    "## Sagemaker Endpoint 설정\n",
    "\n",
    "- Llama2 70B chat 모델을 us-east-1 리전에 배포했다고 가정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "83408918-dcba-4580-91f9-d04ab46016b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_name = None\n",
    "region = 'us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "183e2f0f-32cc-43bf-8daa-b6c745fe37a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(\n",
    "    profile_name=profile_name,\n",
    "    region_name=region,\n",
    ")\n",
    "smclient = boto3.client('sagemaker')\n",
    "smsession = Session(boto_session=boto_session, sagemaker_client=smclient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d557253e-9843-48de-9639-5245bc468df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'jumpstart-dft-meta-textgeneration-llama-2-70b-f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fb9b59a6-ff2e-4db3-8c44-70dd728e88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    sagemaker_session=smsession,\n",
    "    serializer=JSONSerializer(content_type='application/json'),\n",
    "    deserializer = JSONDeserializer(accept='application/json'),\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f23916-06aa-4611-9182-198973fa4964",
   "metadata": {},
   "source": [
    "- 청크로 나누고 정보를 추출한다.\n",
    "- 프롬프트에서 JSON 형태로 결과를 출력하도록 유도한다.\n",
    "- 누락되면 안되는 정보를 누락하지 않도록, Chain of thought 방식으로 표현할 정보를 조정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3ad74533-a60c-4fb3-8082-35557a32a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Extract relevant information from the user text to build a topic model. \\\n",
    "The user text is enclosed in tripple backticks (```).\n",
    "Only output the JSON object, with nothing else.\n",
    "\n",
    "Follow these steps to extract information from the user text.\n",
    "\n",
    "Step 1: List informative keywords that helps to understand the text.\n",
    "\n",
    "Step 2: If the text contains informative name of entities, List them.\n",
    "\n",
    "Step 3: Provide summary of the text in 50 words. \\\n",
    "The summary should containing as many extracted informations in the previous step as possible. \\\n",
    "The information must not contain any code. \\\n",
    "Do not provide any sample code in the information.\n",
    "\n",
    "Provide response as a JSON object with the following schema:\n",
    "{\"keywords\": <step 1 reasoning>, \"entities\": <step 2 reasoning>, \"summary\": <step 3 reasoning>}\n",
    "\"\"\".strip()\n",
    "\n",
    "def _chunk_inputs(chunk):\n",
    "    return [[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f'```{chunk}```'},\n",
    "    ]]\n",
    "\n",
    "def extract_info(chunk):\n",
    "    payload = {\n",
    "        \"inputs\": _chunk_inputs(chunk),\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"top_p\": 0.9,\n",
    "            \"temperature\": 0.01,\n",
    "        }\n",
    "    }\n",
    "    response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "    resp_text = response[0]['generation']['content']\n",
    "    try:\n",
    "        return json.loads(resp_text)\n",
    "    except:\n",
    "        print(resp_text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5989997f-ff71-4ab2-bb8b-0940fbfc2a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start extracting info from doc_id: 0, length: 9100, num_tokens: 1723, chunk_size: 512\n",
      "chunk-0(2810), chunk-1(2810), chunk-2(2514), chunk-3(1257), chunks: 4...\n",
      "start extracting info from doc_id: 1, length: 16040, num_tokens: 3695, chunk_size: 615\n",
      "chunk-0(2610), chunk-1(2538), chunk-2(2590), chunk-3(2759), chunk-4(2756), chunk-5(2777), chunk-6(530), chunks: 7...\n",
      "start extracting info from doc_id: 2, length: 4801, num_tokens: 1098, chunk_size: 512\n",
      "chunk-0(2334), chunk-1(2205), chunk-2(412), chunks: 3...\n",
      "CPU times: user 189 ms, sys: 25.3 ms, total: 215 ms\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "\n",
    "doc_infos = []\n",
    "for doc_id, doc in enumerate(docs):\n",
    "    # try to split into ~6 chunks\n",
    "    encoding = tiktoken.encoding_for_model(model_name)\n",
    "    num_tokens = len(encoding.encode(doc))\n",
    "    chunk_size = max(512, num_tokens//6)\n",
    "    print(f'start extracting info from doc_id: {doc_id}, length: {len(doc)}, num_tokens: {num_tokens}, chunk_size: {chunk_size}')\n",
    "    \n",
    "    splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "        model_name=model_name,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=20,\n",
    "    )\n",
    "    each_info = []\n",
    "    for chunk_idx, chunk in enumerate(splitter.split_text(doc)):\n",
    "        print(f'chunk-{chunk_idx}({len(chunk)}), ', end='')\n",
    "        info = extract_info(chunk)\n",
    "        if info is None:\n",
    "            print(f'invalid output. ', end='')\n",
    "        else:\n",
    "            each_info.append(info)\n",
    "    print(f'chunks: {len(each_info)}...')\n",
    "    \n",
    "    doc_infos.append(each_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a77ff5-e58f-4f8a-af0a-9bf6a82e3a63",
   "metadata": {},
   "source": [
    "# 3. Save to file\n",
    "\n",
    "- 각 청크단위의 정보를 하나로 합쳐셔 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6fa9d4bc-8d29-4723-a84f-3bb0789071c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime('%Y%m%dT%H%M')\n",
    "with open(f'./dataset-{now}.jsonl', 'w') as fp:\n",
    "    for idx, doc_info in enumerate(doc_infos):\n",
    "        keywords = []\n",
    "        entities = []\n",
    "        summary = []\n",
    "        for el in doc_info:\n",
    "            keywords.extend(el['keywords'])\n",
    "            entities.extend(el['entities'])\n",
    "            summary.append(el['summary'])\n",
    "        D = {\n",
    "            'keywords': list(set(keywords)),\n",
    "            'entities': list(set(entities)),\n",
    "            'summary': '\\n'.join(summary),\n",
    "            'url': urls[idx],\n",
    "        }\n",
    "        fp.write(f'{json.dumps(D)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499db809-b1ad-4722-9f34-e2688d1c40a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
