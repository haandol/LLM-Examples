{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79fd766-62b7-4d73-8a8e-b4bc099f2554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
    "!pip -q install datasets sentencepiece \n",
    "!pip -q install bitsandbytes==0.38.0.post2 accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c813872-5892-4eee-b459-5007fc4a2c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 25 10:25:14 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   22C    P8    21W / 300W |      0MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11291735-57fe-42a7-861a-14ab7a666e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "from random import choice\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cdef9a-26e3-4053-b1e1-617fc5af3490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('api')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logHandler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(logHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb45c7f0-c3db-4d93-8775-ce3853b2a4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_model(model_name: str, cache_dir: str = None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "        low_cpu_mem_usage=True,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df2ba300-6090-44e3-93b8-4dba370eeac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so.11.0'), PosixPath('/home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006474018096923828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49f3178695e44619724c44cd5738329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'TheBloke/wizardLM-7B-HF'\n",
    "# model_name = 'ehartford/Wizard-Vicuna-7B-Uncensored'\n",
    "cache_dir = '/home/ec2-user/SageMaker/.cache'\n",
    "tokenizer, model = setup_model(model_name, cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cf6ed3a-17c7-4626-b200-4db4bde91fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 25 10:25:23 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   25C    P0    61W / 300W |   7967MiB / 23028MiB |     19%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     20158      C   ...vs/pytorch_p39/bin/python     7965MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2742574-4989-40f4-b91c-5150c900ae25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEMPLATE = '''\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{user_input}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1314a6bb-c998-4b56-a4b2-79a5a0741787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModelForCausalLM,\n",
    "    prompt: str,\n",
    "    top_k = 0,\n",
    "    top_p = 0.95,\n",
    "    max_new_tokens = 512,\n",
    "    temperature = 0.1,\n",
    "):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        gen_tokens = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens, \n",
    "            num_return_sequences=1,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.15,\n",
    "        )\n",
    "    gen_token = choice(gen_tokens)\n",
    "    generation = tokenizer.decode(gen_token, skip_special_tokens=True)[len(prompt):]\n",
    "    return generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16238e64-3f4b-40a6-850f-f50416f14373",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a081560a-5a13-4572-9053-f5eaa41e9528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "This is awesome! // Positive.\n",
    "This is bad! // Negative.\n",
    "Wow that movie was rad! // Positive.\n",
    "What a horrible show! // \n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ea8cd587-40ab-4b9a-a993-25e4ec556a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@gen:  Negative.\n",
      "CPU times: user 561 ms, sys: 6 µs, total: 561 ms\n",
      "Wall time: 560 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logger.debug(prompt)\n",
    "generation = generate(tokenizer, model, prompt)\n",
    "logger.info(f'@@gen: {generation}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63330a78-a05e-4cea-8ce0-ea9c8c05bdd1",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8afaa08d-4df1-400b-a3e1-483cee7bc9b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "Antibiotics are a type of medication used to treat bacterial infections.\n",
    "They work by either killing the bacteria or preventing them from reproducing, allowing the body’s immune system to fight off the infection.\n",
    "Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously.\n",
    "They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.\n",
    "\n",
    "Explain the above in one sentence:\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f1b83af4-c6e8-4656-83fb-b2631c08e8d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@gen:  Antibiotics are medications that combat bacterial infections by killing or hindering their growth, allowing the immune system to fight off the illness.\n",
      "CPU times: user 4.79 s, sys: 4.01 ms, total: 4.8 s\n",
      "Wall time: 4.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logger.debug(prompt)\n",
    "generation = generate(tokenizer, model, prompt)\n",
    "logger.info(f'@@gen: {generation}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82c6903a-ef40-4f44-84b7-46a2f9f9090f",
   "metadata": {},
   "source": [
    "## Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "cd503388-1b78-46b9-bee2-3442a261e90d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and analysis.\n",
    "They should also indicate which LLMs were used.\n",
    "This will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases, inaccuracies and improper source crediting.\n",
    "Likewise, scientific journals should be transparent about their use of LLMs, for example when selecting submitted manuscripts.\n",
    "\n",
    "Mention the large language model based product mentioned in the paragraph above:\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "865e5a29-e5bc-41b8-8018-4232bcc2aa4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@gen:  \"ChatGPT\"\n",
      "CPU times: user 1.03 s, sys: 0 ns, total: 1.03 s\n",
      "Wall time: 1.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logger.debug(prompt)\n",
    "generation = generate(tokenizer, model, prompt)\n",
    "logger.info(f'@@gen: {generation}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0a72a59-8a3a-4126-ac42-2b36c364b04a",
   "metadata": {},
   "source": [
    "## Zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "aa35421e-46f0-4245-a81b-c15d0a84c8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "Classify the text into neutral, negative or positive. \n",
    "\n",
    "Text: I think the vacation is okay.\n",
    "Sentiment:\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c1d2a55e-19e9-4007-869b-04cedbcce9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@gen:  Neutral\n",
      "CPU times: user 560 ms, sys: 33 µs, total: 560 ms\n",
      "Wall time: 559 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logger.debug(prompt)\n",
    "generation = generate(tokenizer, model, prompt)\n",
    "logger.info(f'@@gen: {generation}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7522df5-7e05-4b04-8ced-469369311c5c",
   "metadata": {},
   "source": [
    "## Few-shot learning\n",
    "- 3개 이상 shot 을 추가해주면 효과적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fb9613a8-fabc-4e64-bca0-845a57e5770f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction='''\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17, 10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16, 11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17, 9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "'''.strip()\n",
    "\n",
    "user_input = '''\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
    "A: \n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9a75b123-5a43-406e-a7c0-e5852a251750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@gen: The answer is True.\n",
      "CPU times: user 851 ms, sys: 3.94 ms, total: 855 ms\n",
      "Wall time: 853 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = TEMPLATE.format(instruction=instruction, user_input=user_input)\n",
    "logger.debug(prompt)\n",
    "generation = generate(tokenizer, model, prompt)\n",
    "logger.info(f'@@gen: {generation}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90c96d0d-cfa2-41ea-8917-d6a82b6b78a0",
   "metadata": {},
   "source": [
    "## One-shot CoT (Chain ot thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "138d916f-df38-44f3-bc14-7b08a21b405e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = '''\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "'''.strip()\n",
    "\n",
    "user_input = '''\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 7, 1.\n",
    "A: \n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2a749f37-a183-4394-b6da-3970b473df90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@gen: Adding all the odd numbers (15, 32, 5, 7, 1) gives 60. However, since there are no additional details provided, I cannot determine if the statement is true or false.\n",
      "CPU times: user 6.75 s, sys: 0 ns, total: 6.75 s\n",
      "Wall time: 6.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = TEMPLATE.format(instruction=instruction, user_input=user_input)\n",
    "logger.debug(prompt)\n",
    "generation = generate(tokenizer, model, prompt, temperature=0.2)\n",
    "logger.info(f'@@gen: {generation}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c3fc115-f278-47cd-b19f-b65ad41fd590",
   "metadata": {},
   "source": [
    "### With `Let's think step by step.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7e51bcec-9688-478c-9fc5-deb1740d90ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = '''\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "'''.strip()\n",
    "\n",
    "user_input = '''\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 7, 1.\n",
    "A: Let's think step by step.\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "79de8f42-4f3a-494d-a2f3-4df934ca22ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@gen: First, we need to find the sum of all the odd numbers in the list. That would be 15 + 32 + 5 + 7 = 60.\n",
      "Then, we need to check if 60 is an even number. It is! Therefore, the answer is True.\n",
      "CPU times: user 8.6 s, sys: 0 ns, total: 8.6 s\n",
      "Wall time: 8.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = TEMPLATE.format(instruction=instruction, user_input=user_input)\n",
    "logger.debug(prompt)\n",
    "generation = generate(tokenizer, model, prompt, temperature=0.4)\n",
    "logger.info(f'@@gen: {generation}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "241dfcda-77c5-4f60-a378-9aead2266498",
   "metadata": {},
   "source": [
    "# PAL (program aided language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "03948787-461c-4369-b79e-d02d92fc698d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "# Q: 2015 is coming in 36 hours. What is the date one week from today in MM/DD/YYYY?\n",
    "# If 2015 is coming in 36 hours, then today is 36 hours before.\n",
    "today = datetime(2015, 1, 1) - relativedelta(hours=36)\n",
    "# One week from today,\n",
    "one_week_from_today = today + relativedelta(weeks=1)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "one_week_from_today.strftime('%m/%d/%Y')\n",
    "\n",
    "# Q: The first day of 2019 is a Tuesday, and today is the first Monday of 2019. What is the date today in MM/DD/YYYY?\n",
    "# If the first day of 2019 is a Tuesday, and today is the first Monday of 2019, then today is 6 days later.\n",
    "today = datetime(2019, 1, 1) + relativedelta(days=6)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "today.strftime('%m/%d/%Y')\n",
    "\n",
    "# Q: The concert was scheduled to be on 06/01/1943, but was delayed by one day to today. What is the date 10 days ago in MM/DD/YYYY?\n",
    "# If the concert was scheduled to be on 06/01/1943, but was delayed by one day to today, then today is one day later.\n",
    "today = datetime(1943, 6, 1) + relativedelta(days=1)\n",
    "# 10 days ago,\n",
    "ten_days_ago = today - relativedelta(days=10)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "ten_days_ago.strftime('%m/%d/%Y')\n",
    "\n",
    "# Q: It is 4/19/1969 today. What is the date 24 hours later in MM/DD/YYYY?\n",
    "# It is 4/19/1969 today.\n",
    "today = datetime(1969, 4, 19)\n",
    "# 24 hours later,\n",
    "later = today + relativedelta(hours=24)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "today.strftime('%m/%d/%Y')\n",
    "\n",
    "# Q: Jane thought today is 3/11/2002, but today is in fact Mar 12, which is 1 day later. What is the date 24 hours later in MM/DD/YYYY?\n",
    "# If Jane thought today is 3/11/2002, but today is in fact Mar 12, then today is 3/1/2002.\n",
    "today = datetime(2002, 3, 12)\n",
    "# 24 hours later,\n",
    "later = today + relativedelta(hours=24)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "later.strftime('%m/%d/%Y')\n",
    "\n",
    "# Q: Jane was born on the last day of Feburary in 2001. Today is her 16-year-old birthday. What is the date yesterday in MM/DD/YYYY?\n",
    "# If Jane was born on the last day of Feburary in 2001 and today is her 16-year-old birthday, then today is 16 years later.\n",
    "today = datetime(2001, 2, 28) + relativedelta(years=16)\n",
    "# Yesterday,\n",
    "yesterday = today - relativedelta(days=1)\n",
    "# The answer formatted with %m/%d/%Y is\n",
    "yesterday.strftime('%m/%d/%Y')\n",
    "\n",
    "# Q: Today is 27 February 2023. I was born exactly 25 years ago. What is the date I was born in MM/DD/YYYY?\n",
    "'''.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "977c37a0-551a-4bdb-94e2-dc3c833d20fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@gen: # If today is 27 February 2023 and you were born exactly 25 years ago, then you were born on 27 February 1998.\n",
      "birthdate = datetime(1998, 2, 27)\n",
      "# The answer formatted with %m/%d/%Y is\n",
      "birthdate.strftime('%m/%d/%Y')\n",
      "CPU times: user 11.8 s, sys: 0 ns, total: 11.8 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logger.debug(prompt)\n",
    "generation = generate(tokenizer, model, prompt, temperature=0.1)\n",
    "logger.info(f'@@gen: {generation}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c246ff16-dbd4-436a-ae63-5f39df7f6781",
   "metadata": {},
   "source": [
    "## Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d4a7f064-8bbc-4961-8230-7e587f429f78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = '''\n",
    "Produce 10 exemplars for sentiment analysis. Examples are categorized as either positive or negative. Produce 2 negative examples and 8 positive examples. Use this format for the examples:\n",
    "Q: <sentence>\n",
    "A: <sentiment>\n",
    "'''.strip()\n",
    "\n",
    "user_input = '''\n",
    "Q: \n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1bfeb56a-cbd8-4bfd-b4c7-b5ded961bb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@gen: \"I am so excited to receive my new job promotion!\"\n",
      "A: Positive\n",
      "\n",
      "Q: \"The traffic on the way to work was terrible today.\"\n",
      "A: Negative\n",
      "\n",
      "Q: \"My favorite restaurant closed down unexpectedly.\"\n",
      "A: Negative\n",
      "\n",
      "Q: \"The sunset tonight was absolutely breathtaking.\"\n",
      "A: Positive\n",
      "\n",
      "Q: \"I got a good deal on a new pair of shoes at the mall today.\"\n",
      "A: Positive\n",
      "\n",
      "Q: \"My boss gave me a great compliment during our meeting today.\"\n",
      "A: Positive\n",
      "\n",
      "Q: \"I had a really bad cold all week, but I'm feeling better now.\"\n",
      "A: Negative\n",
      "\n",
      "Q: \"The weather forecast predicted rain for tomorrow, but it looks like it might clear up later in the day.\"\n",
      "A: Neutral\n",
      "CPU times: user 25.8 s, sys: 3.88 ms, total: 25.8 s\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = TEMPLATE.format(instruction=instruction, user_input=user_input)\n",
    "logger.debug(prompt)\n",
    "generation = generate(tokenizer, model, prompt, temperature=0.8)\n",
    "logger.info(f'@@gen: {generation}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f65a8d9b-9808-4793-9471-e16a1000c496",
   "metadata": {},
   "source": [
    "## Generate Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4d9d7cc4-2580-46a0-8296-2456f5b9f5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "SELECT students.StudentId, students.StudentName\n",
    "FROM students\n",
    "INNER JOIN departments\n",
    "ON students.DepartmentId = departments.DepartmentId\n",
    "WHERE departments.DepartmentName = 'Computer Science';\n",
    "\n",
    "Explain the above SQL statement.\n",
    "'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3bbb0815-1b19-466b-9624-8ebb6c4cb543",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@gen:  \n",
      "This SQL statement selects the StudentID and StudentName from the Students table, and then joins it with the Departments table on the DepartmentID field. The WHERE clause filters the results to only include rows where the DepartmentName is \"Computer Science\". This query will return a list of all Computer Science students along with their names.\n",
      "CPU times: user 8.93 s, sys: 3.96 ms, total: 8.93 s\n",
      "Wall time: 8.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "logger.debug(prompt)\n",
    "generation = generate(tokenizer, model, prompt, temperature=0.1)\n",
    "logger.info(f'@@gen: {generation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8d076-5646-4120-a2f9-7c94cf932ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
